{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5ec422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T03:54:00.072730Z",
     "start_time": "2023-01-08T03:53:59.032974Z"
    },
    "id": "5e5ec422"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cqfwpACuJ_-j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "cqfwpACuJ_-j",
    "outputId": "eaf6e231-7e2e-48f5-a4eb-2b12aac5cae1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-abc0837b-8baf-4e30-9fef-72f6ac71e4ca\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-abc0837b-8baf-4e30-9fef-72f6ac71e4ca\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sherloch holmes.txt to sherloch holmes.txt\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "f = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "x5VZ1kz51IP6",
   "metadata": {
    "id": "x5VZ1kz51IP6"
   },
   "outputs": [],
   "source": [
    "f = open(\"sherloch holmes.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "for i in f:\n",
    "    lines.append(i)\n",
    "\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "  data = ' '. join(lines) \n",
    "\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')\n",
    "\n",
    "data = data.split()\n",
    "data = ' '.join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "PlsMvAMx1dfU",
   "metadata": {
    "id": "PlsMvAMx1dfU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Jo9fOU9C1IMr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jo9fOU9C1IMr",
    "outputId": "8d99648f-6460-43d4-ee16-96637a8f68cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[142, 4680, 1, 986, 5, 125, 33, 46, 556, 2164, 2165, 27, 987, 14, 22]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "zJa4V3xR1IGB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJa4V3xR1IGB",
    "outputId": "e04dd555-00f0-4a0d-9c00-b4bcfc125156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8624\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "C-WKJ7jx1H7L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-WKJ7jx1H7L",
    "outputId": "f568f683-770e-4854-d969-50b3bf4f2aed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  108955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-c43534fdb19a>:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences = np.array(sequences)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list([]), list([142, 4680, 1, 986, 5]),\n",
       "       list([4680, 1, 986, 5, 125]), list([1, 986, 5, 125, 33]),\n",
       "       list([986, 5, 125, 33, 46]), list([5, 125, 33, 46, 556]),\n",
       "       list([125, 33, 46, 556, 2164]), list([33, 46, 556, 2164, 2165]),\n",
       "       list([46, 556, 2164, 2165, 27]), list([556, 2164, 2165, 27, 987])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(3, len(sequence_data)):\n",
    "    words = sequence_data[i-4:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "FVJ0jhaw1zCM",
   "metadata": {
    "id": "FVJ0jhaw1zCM"
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    if len(i) < 4:\n",
    "      continue\n",
    "    X.append(i[0:4])\n",
    "    y.append(i[-1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "A8aaL9TuIkNu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8aaL9TuIkNu",
    "outputId": "4dfe7946-db56-4079-f97c-3cd29d0e27f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 142, 4680,    1,  986])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "OhN_6QNY11dS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OhN_6QNY11dS",
    "outputId": "74f40672-bd70-4bce-c1ad-8852798b59c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "uDBat8kw2Jf9",
   "metadata": {
    "id": "uDBat8kw2Jf9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "w9a1-UfJDaBj",
   "metadata": {
    "id": "w9a1-UfJDaBj"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from keras import backend as K\n",
    "\n",
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.pow(2.0, cross_entropy)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "oQIoQREz2AJc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQIoQREz2AJc",
    "outputId": "bbdf2c30-ed45-4f26-d076-b222101d9f0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 10)             86240     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 4, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8624)              8632624   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,767,864\n",
      "Trainable params: 21,767,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=len(X[0])))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rWG7HpuU2bFw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rWG7HpuU2bFw",
    "outputId": "3d8d7d97-3497-4ad2-fc0a-032f75195313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "213/213 [==============================] - 34s 112ms/step - loss: 6.7696 - perplexity: 606.7615\n",
      "Epoch 2/30\n",
      "213/213 [==============================] - 16s 76ms/step - loss: 6.3853 - perplexity: 434.6445\n",
      "Epoch 3/30\n",
      "213/213 [==============================] - 13s 63ms/step - loss: 6.0865 - perplexity: 434.1695\n",
      "Epoch 4/30\n",
      "213/213 [==============================] - 13s 63ms/step - loss: 5.8215 - perplexity: 329.1391\n",
      "Epoch 5/30\n",
      "213/213 [==============================] - 14s 63ms/step - loss: 5.5543 - perplexity: 270.0941\n",
      "Epoch 6/30\n",
      "213/213 [==============================] - 14s 66ms/step - loss: 5.3032 - perplexity: 182.4940\n",
      "Epoch 7/30\n",
      "213/213 [==============================] - 14s 65ms/step - loss: 5.0683 - perplexity: 137.4905\n",
      "Epoch 8/30\n",
      "213/213 [==============================] - 14s 65ms/step - loss: 4.8285 - perplexity: 102.4115\n",
      "Epoch 9/30\n",
      "213/213 [==============================] - 14s 68ms/step - loss: 4.5759 - perplexity: 7656.4272\n",
      "Epoch 10/30\n",
      "213/213 [==============================] - 14s 65ms/step - loss: 4.3050 - perplexity: 248.7501\n",
      "Epoch 11/30\n",
      "213/213 [==============================] - 14s 63ms/step - loss: 4.0225 - perplexity: 67.6869\n",
      "Epoch 12/30\n",
      "213/213 [==============================] - 14s 66ms/step - loss: 3.7326 - perplexity: 42.0377\n",
      "Epoch 13/30\n",
      "213/213 [==============================] - 13s 63ms/step - loss: 3.4399 - perplexity: 33.5174\n",
      "Epoch 14/30\n",
      "213/213 [==============================] - 14s 64ms/step - loss: 3.1529 - perplexity: 27.0421\n",
      "Epoch 15/30\n",
      "213/213 [==============================] - 14s 64ms/step - loss: 2.8725 - perplexity: 22.5818\n",
      "Epoch 16/30\n",
      "213/213 [==============================] - 14s 66ms/step - loss: 2.5928 - perplexity: 18.3397\n",
      "Epoch 17/30\n",
      "213/213 [==============================] - 14s 67ms/step - loss: 2.3122 - perplexity: 14.8745\n",
      "Epoch 18/30\n",
      "213/213 [==============================] - 14s 65ms/step - loss: 2.0429 - perplexity: 12.3702\n",
      "Epoch 19/30\n",
      "213/213 [==============================] - 14s 64ms/step - loss: 1.7771 - perplexity: 9.8052\n",
      "Epoch 20/30\n",
      "213/213 [==============================] - 14s 64ms/step - loss: 1.5177 - perplexity: 8.8135\n",
      "Epoch 21/30\n",
      "213/213 [==============================] - 14s 64ms/step - loss: 1.2647 - perplexity: 5.9803\n",
      "Epoch 22/30\n",
      "213/213 [==============================] - 14s 65ms/step - loss: 1.0296 - perplexity: 4.9288\n",
      "Epoch 23/30\n",
      "213/213 [==============================] - 14s 64ms/step - loss: 0.8169 - perplexity: 4.1111\n",
      "Epoch 24/30\n",
      "213/213 [==============================] - 14s 65ms/step - loss: 0.6361 - perplexity: 9.8709\n",
      "Epoch 25/30\n",
      "213/213 [==============================] - 14s 65ms/step - loss: 0.4914 - perplexity: 4.7385\n",
      "Epoch 26/30\n",
      "213/213 [==============================] - 14s 64ms/step - loss: 0.3859 - perplexity: 15.5410\n",
      "Epoch 27/30\n",
      "213/213 [==============================] - 14s 65ms/step - loss: 0.3078 - perplexity: 38022.5234\n",
      "Epoch 28/30\n",
      "213/213 [==============================] - 14s 64ms/step - loss: 0.2593 - perplexity: 2.2718\n",
      "Epoch 29/30\n",
      "213/213 [==============================] - 14s 64ms/step - loss: 0.2263 - perplexity: 2.0645\n",
      "Epoch 30/30\n",
      "213/213 [==============================] - 14s 65ms/step - loss: 0.2052 - perplexity: 2.3558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9a02b3ac0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[perplexity])\n",
    "model.fit(X, y, epochs=30, batch_size= 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "L4vf6hzGyOUp",
   "metadata": {
    "id": "L4vf6hzGyOUp"
   },
   "outputs": [],
   "source": [
    "model.save('sherlock_holmes.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "332e2682",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-08T03:53:59.078Z"
    },
    "id": "332e2682"
   },
   "outputs": [],
   "source": [
    "def pred(model, tokenizer, text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    sequence = np.array(sequence)\n",
    "    preds = np.argmax(model.predict(sequence, verbose = 0))\n",
    "    predicted_word = \"\"\n",
    "\n",
    "    for key, value in tokenizer.word_index.items():\n",
    "        if value == preds:\n",
    "            predicted_word = key\n",
    "            break\n",
    "    \n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f4160b1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-08T03:53:59.082Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f4160b1",
    "outputId": "847974e7-c9b8-491c-f460-58e0e177c484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Enter your line: this may save us a visit\n",
      "\tto brixton road whispered holmes come with me and we will see what is to \n",
      "\n",
      "\n",
      "Enter your line: Whatever were you doing with\n",
      "\tthat bird jem ’ says she ‘well ’ said i ‘you may absolutely depend upon \n",
      "\n",
      "\n",
      "Enter your line: Life is infinitely stranger than\n",
      "\tanything which the mind of man could invent we would not dare to conceive the \n",
      "\n",
      "\n",
      "Enter your line: 0\n",
      "Execution completed.....\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "  text = input(\"\\n\\nEnter your line: \")\n",
    "  if text == \"0\":\n",
    "      print(\"Execution completed.....\")\n",
    "      break  \n",
    "  else:\n",
    "      try:\n",
    "          ans = ''\n",
    "          for i in range(15): \n",
    "            predicted_word = pred(model, tokenizer, text.split()[-4:])\n",
    "            # if stopwords(predicted_word):\n",
    "            #   print(\"Reached End Of Sentence\")\n",
    "            #   break\n",
    "            ans += predicted_word+' '\n",
    "            text += ' '+ predicted_word\n",
    "          print('\\t' + ans)\n",
    "      except Exception as e:\n",
    "        print(\"Error occurred: \",e)\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
